import random

import nolds
import numpy as np
import pandas as pd
import statsmodels.stats.weightstats as st
import torch


__all__ = ['set_random_seed',
           'results_summary',
           'times_summary',
           'generate_exchange_index',
           'cal_lyapunov_exponent']


def set_random_seed(seed):
    """ set the random number seed for reproducing the results. """
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def _nan_to_value(ilist, value=.5):
    """ change the nan to a defined value. """
    olist = []
    for i in ilist:
        if np.isnan(i):
            olist.append(value)
        else:
            olist.append(i)
    return olist
    
    
def results_summary(df, acc_train=False):
    """ summarized the results of training. """
    key_acc = 'train acc' if acc_train else 'test acc'
    
    loss_BP = df[df.method == 'BP'].loc[:, 'train loss']
    loss_CBP = df[df.method == 'CBP'].loc[:, 'train loss']
    loss_pvalue = st.ttest_ind(loss_BP, loss_CBP, alternative='larger')[1]
    acc_BP = df[df.method == 'BP'].loc[:, key_acc]
    acc_CBP = df[df.method == 'CBP'].loc[:, key_acc]
    acc_pvalue = st.ttest_ind(acc_BP, acc_CBP, alternative='smaller')[1]
    loss_pvalue, acc_pvalue = _nan_to_value([loss_pvalue, acc_pvalue])
    
    sum_df = pd.DataFrame(
        {'method': ['BP', 'CBP'],
         'loss_mean': [loss_BP.mean(), loss_CBP.mean()],
         'loss_std': [loss_BP.std(), loss_CBP.std()],
         'loss_pvalue': [loss_pvalue, 'None'],
         'acc_mean': [acc_BP.mean(), acc_CBP.mean()],
         'acc_std': [acc_BP.std(), acc_CBP.std()],
         'acc_pvalue': [acc_pvalue, 'None'],
         }
    )
    return sum_df


def times_summary(df):
    """ summarized the time information of training. """
    train_BP = df[df.method == 'BP'].iloc[:, 2]
    train_CBP = df[df.method == 'CBP'].iloc[:, 2]
    train_pvalue = st.ttest_ind(train_BP, train_CBP, alternative='larger')[1]
    acc1_BP = df[df.method == 'BP'].iloc[:, 3]
    acc1_CBP = df[df.method == 'CBP'].iloc[:, 3]
    acc1_pvalue = st.ttest_ind(acc1_BP, acc1_CBP, alternative='larger')[1]
    acc2_BP = df[df.method == 'BP'].iloc[:, 4]
    acc2_CBP = df[df.method == 'CBP'].iloc[:, 4]
    acc2_pvalue = st.ttest_ind(acc2_BP, acc2_CBP, alternative='larger')[1]
    
    train_pvalue, acc1_pvalue, acc2_pvalue = _nan_to_value([train_pvalue, acc1_pvalue, acc2_pvalue])
    
    sum_df = pd.DataFrame(
        {'method': ['BP', 'CBP'],
         'train_mean': [train_BP.mean(), train_CBP.mean()],
         'train_std': [train_BP.std(), train_CBP.std()],
         'train_pvalue': [train_pvalue, 'None'],
         'acc1_mean': [acc1_BP.mean(), acc1_CBP.mean()],
         'acc1_std': [acc1_BP.std(), acc1_CBP.std()],
         'acc1_pvalue': [acc1_pvalue, 'None'],
         'acc2_mean': [acc2_BP.mean(), acc2_CBP.mean()],
         'acc2_std': [acc2_BP.std(), acc2_CBP.std()],
         'acc2_pvalue': [acc2_pvalue, 'None'],
         }
    )
    return sum_df


def generate_exchange_index(n_base, n_repeat):
    """ exchange the index. """
    index1 = np.zeros(n_base * n_repeat)
    index2 = np.zeros(n_base * n_repeat)
    for i in range(n_repeat):
        index1[i * n_base : (i + 1) * n_base] = np.arange(2 * i * n_base, (2 * i + 1) * n_base)
        index2[i * n_base : (i + 1) * n_base] = np.arange((2 * i + 1) * n_base, (2 * i + 2) * n_base)
    return np.hstack((index1, index2)).astype(int)


def cal_lyapunov_exponent(ws_lists, layer_idx=0, count_idx=0):
    """ calculate the Lyapunov exponent with nolds.

    :param ws_lists: weights lists generated by cbpy.debug_chaos()
    :param layer_idx: index of layer
    :param count_idx: index of count
    :return: nolds_les: Lyapunov exponents calculated by nolds
    """
    w_lists = [[w[layer_idx][count_idx] for w in ws] for ws in ws_lists]
    nolds_les = [nolds.lyap_e(l)[0] for l in w_lists]
    return nolds_les